{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMh7FYUKcx6jiuhGeTtnzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-oth/dst_assessment2/blob/main/ComparingSaliencyMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitatively Comparing Saliency Methods: The AOPC\\_MoRF Metric\n",
        "\n",
        "In this project each group member implemented a different post-hoc interpretation method\n",
        "(Grad-CAM, SmoothGrad, LRP) on the same CNN trained to classify brain tumour MRI slices.\n",
        "To compare these explanations in a principled way we require a fidelity metric: a measure\n",
        "of how well a saliency map identifies the specific image regions that truly drive the model’s\n",
        "prediction. Importantly, the goal here is explainability fidelity, not segmentation accuracy.\n",
        "We are not judging how well a method outlines the tumour anatomically, but whether it\n",
        "correctly highlights the regions that this particular CNN relies on.\n",
        "\n",
        "---\n",
        "\n",
        "### What is a saliency map?\n",
        "\n",
        "A saliency map is a matrix, aligned with the input image, that assigns an *importance score*  \n",
        "to each pixel or region with respect to a particular prediction. Formally, for an image $x$ and\n",
        "model output $f(x)$, a saliency method produces a map:\n",
        "\n",
        "$$\n",
        "R = \\{ R_i \\mid i \\in \\text{pixels of } x \\},\n",
        "$$\n",
        "\n",
        "where $R_i$ measures how influential pixel $i$ is for the model’s chosen class.  \n",
        "Different methods compute $R$ differently:\n",
        "\n",
        "- **Grad-CAM**: weights feature maps by the gradient of the target class and upsamples.\n",
        "- **SmoothGrad**: averages gradients under noise to reduce visual noise.\n",
        "- **LRP**: redistributes the output score backwards according to conservation rules.\n",
        "\n",
        "These maps are qualitative visualisations, but we require a **quantitative** way to assess how\n",
        "faithfully they reflect what the model actually uses.\n",
        "\n",
        "---\n",
        "\n",
        "### The AOPC\\_MoRF Metric\n",
        "\n",
        "To evaluate fidelity, we use AOPC\\_MoRF (Area Over the Perturbation Curve – Most Relevant First.\n",
        "The core idea is simple:\n",
        "\n",
        "> If a saliency map is faithful, then removing the pixels it marks as “important” should quickly\n",
        "> reduce the model’s confidence.\n",
        "\n",
        "Given an image $x$, its original score $f(x)$, and a ranking of regions from most to least\n",
        "important, we progressively **delete** (perturb) the top–$k$ regions and record the model's\n",
        "confidence on the modified images $x^{(k)}$.\n",
        "\n",
        "The AOPC\\_MoRF score for a single image is:\n",
        "\n",
        "$$\n",
        "\\text{AOPC}_{\\text{MoRF}}(x)\n",
        "= \\frac{1}{K} \\sum_{k=1}^{K} \\big[ f(x) - f(x^{(k)}) \\big],\n",
        "$$\n",
        "\n",
        "where $K$ is the number of perturbation steps.  \n",
        "A higher score indicates that deleting the most relevant regions causes a rapid drop in the class\n",
        "score, meaning the explanation is more faithful to the model’s behaviour.\n",
        "\n",
        "We adopt standard perturbations such as zeroing or mean-value replacement of selected\n",
        "regions, following reproducible approaches in the literature.\n",
        "\n",
        "---\n",
        "\n",
        "### Why we chose AOPC\\_MoRF\n",
        "\n",
        "Following the analysis in *Sanity Checks for Saliency Metrics* (Tomsett et al., 2020), we select\n",
        "AOPC\\_MoRF as our primary comparison metric for three reasons:\n",
        "\n",
        "1. **Direct fidelity assessment**  \n",
        "   It directly tests whether the highlighted regions are genuinely decision-critical for the CNN.\n",
        "\n",
        "2. **Method comparability**  \n",
        "   Grad-CAM, SmoothGrad and LRP all produce scalar importance maps, allowing a unified\n",
        "   ranking and deletion-based evaluation.\n",
        "\n",
        "3. **Relative robustness in prior research**  \n",
        "   Among the evaluated metrics, AOPC\\_MoRF demonstrated the most stable cross-method\n",
        "   behaviour, whereas alternatives such as LeRF and single-pixel faithfulness exhibited\n",
        "   sensitivity and instability.\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations and interpretation caution\n",
        "\n",
        "It is important to emphasise that AOPC\\_MoRF is not a universally reliable or perfectly stable\n",
        "metric. The Sanity Checks paper highlights:\n",
        "\n",
        "- high variance across images,\n",
        "- sensitivity to the perturbation strategy,\n",
        "- moderate inter-method reliability,\n",
        "- and the risk of evaluating off-manifold images.\n",
        "\n",
        "This metric evaluates fidelity to the model, not correctness relative to ground\n",
        "truth. A saliency map can score highly while still highlighting spurious regions the model has\n",
        "learned. This is an inherent limitation of post-hoc explainability and not specific to any one\n",
        "method.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "Given these considerations, AOPC\\_MoRF serves as a practical and defensible choice for\n",
        "comparing our three saliency approaches in the tumour-classification context. It measures how\n",
        "tightly an explanation aligns with the model’s actual decision process, while acknowledging that\n",
        "no single metric captures explanation quality in a complete or reliable manner. We therefore use\n",
        "AOPC\\_MoRF as our **primary fidelity metric**, interpreted with transparency about its limitations\n",
        "and with a focus on relative — not absolute — comparisons.\n"
      ],
      "metadata": {
        "id": "SWJ6pEWyEsxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM Implementation:"
      ],
      "metadata": {
        "id": "vWxDr03vGNEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##now we implement the Method for Grad-CAM"
      ],
      "metadata": {
        "id": "tUMWrtbnGHm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "cv36WG9IGRv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmoothGrad Implementation:"
      ],
      "metadata": {
        "id": "-m_freq0GoiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SmoothGrad Implementation:\n"
      ],
      "metadata": {
        "id": "FpznWOLeGYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "rfmbFxmuGeXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LRP Implementation:"
      ],
      "metadata": {
        "id": "TN6n63OgGsPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LRP"
      ],
      "metadata": {
        "id": "36NU7lpQGjtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "HYTV8nu_GxKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison:"
      ],
      "metadata": {
        "id": "dHpsAGKPGzKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### all the metrics in one output"
      ],
      "metadata": {
        "id": "YMLCk2vQG2IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:"
      ],
      "metadata": {
        "id": "MTVtZtPqG63d"
      }
    }
  ]
}
