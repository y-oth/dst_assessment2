{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU2/SFb/aViaH/qseLaXYI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-oth/dst_assessment2/blob/main/report/comparingsaliencymethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitatively Comparing Saliency Methods: The AOPC\\_MoRF Metric\n",
        "\n",
        "In this project each group member implemented a different post-hoc interpretation method [1]\n",
        "(Grad-CAM, SmoothGrad, LRP, Integrated Gradients) on the same CNN trained to classify brain tumour MRI slices.\n",
        "To compare these explanations in a principled way we require a fidelity metric: a measure\n",
        "of how well a saliency map identifies the specific image regions that truly drive the model’s\n",
        "prediction [2]. Importantly, the goal here is explainability fidelity, not segmentation accuracy.\n",
        "We are not judging how well a method outlines the tumour anatomically, but whether it\n",
        "correctly highlights the regions that this particular CNN relies on [3].\n",
        "\n",
        "---\n",
        "\n",
        "### What is a saliency map?\n",
        "\n",
        "A saliency map is a matrix, aligned with the input image, that assigns an *importance score*  \n",
        "to each pixel or region with respect to a particular prediction. Formally, for an image $x$ and\n",
        "model output $f(x)$, a saliency method produces a map:\n",
        "\n",
        "$$\n",
        "R = \\{ R_i \\mid i \\in \\text{pixels of } x \\},\n",
        "$$\n",
        "\n",
        "where $R_i$ measures how influential pixel $i$ is for the model’s chosen class [4].  \n",
        "Different methods compute $R$ differently:\n",
        "\n",
        "- **Grad-CAM**: weights feature maps by the gradient of the target class and upsamples.\n",
        "- **SmoothGrad**: averages gradients under noise to reduce visual noise.\n",
        "- **LRP**: redistributes the output score backwards according to conservation rules.\n",
        "\n",
        "These maps are qualitative visualisations, but we require a **quantitative** way to assess how\n",
        "faithfully they reflect what the model actually uses.\n",
        "\n",
        "---\n",
        "\n",
        "### The AOPC\\_MoRF Metric\n",
        "\n",
        "To evaluate fidelity, we use AOPC\\_MoRF (Area Over the Perturbation Curve – Most Relevant First [2],[5].\n",
        "The core idea is simple:\n",
        "\n",
        "> If a saliency map is faithful, then removing the pixels it marks as “important” should quickly\n",
        "> reduce the model’s confidence.\n",
        "\n",
        "Given an image $x$, its original score $f(x)$, and a ranking of regions from most to least\n",
        "important, we progressively **delete** (perturb) the top–$k$ regions and record the model's\n",
        "confidence on the modified images $x^{(k)}$.\n",
        "\n",
        "The AOPC\\_MoRF score for a single image is:\n",
        "\n",
        "$$\n",
        "\\text{AOPC}_{\\text{MoRF}}(x)\n",
        "= \\frac{1}{K} \\sum_{k=1}^{K} \\big[ f(x) - f(x^{(k)}) \\big],\n",
        "$$\n",
        "\n",
        "where $K$ is the number of perturbation steps.  \n",
        "A higher score indicates that deleting the most relevant regions causes a rapid drop in the class\n",
        "score, meaning the explanation is more faithful to the model’s behaviour.\n",
        "\n",
        "We adopt a standard perturbation function - we replace values of pixel with zero [6].\n",
        "\n",
        "We then take the mean of this $AOPC_{MoRF}(x)$ metric over a single batch in the dataset due to limitations in computational power.\n",
        "\n",
        "---\n",
        "\n",
        "### Why we chose AOPC\\_MoRF\n",
        "\n",
        "Following the analysis in *Sanity Checks for Saliency Metrics* (Tomsett et al., 2020)[2], we select\n",
        "AOPC\\_MoRF as our primary comparison metric for three reasons:\n",
        "\n",
        "1. **Direct fidelity assessment**  \n",
        "   It directly tests whether the highlighted regions are genuinely decision-critical for the CNN.\n",
        "\n",
        "2. **Method comparability**  \n",
        "   Grad-CAM, SmoothGrad and LRP all produce scalar importance maps, allowing a unified\n",
        "   ranking and deletion-based evaluation.\n",
        "\n",
        "3. **Relative robustness in prior research**  \n",
        "   Among the evaluated metrics, AOPC\\_MoRF demonstrated the most stable cross-method\n",
        "   behaviour, whereas alternatives such as LeRF (Least Relevant first) and single-pixel faithfulness exhibited\n",
        "   sensitivity and instability .\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations and interpretation caution\n",
        "\n",
        "It is important to emphasise that AOPC\\_MoRF is not a universally reliable or perfectly stable\n",
        "metric. The paper highlights:\n",
        "\n",
        "- high variance across images,\n",
        "- sensitivity to the perturbation strategy (mean value pretubation or zero value pertubation),\n",
        "- inconsistency between rankings of saliency methods image-by-image.\n",
        "\n",
        "This metric evaluates fidelity to the model, not correctness relative to ground\n",
        "truth. A saliency map can score highly while still highlighting spurious regions the model has\n",
        "learned. This is an inherent limitation of post-hoc explainability and not specific to any one\n",
        "method.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "Given these considerations, AOPC\\_MoRF serves as a practical and defensible choice for\n",
        "comparing our three saliency approaches in the tumour-classification context. It measures how\n",
        "tightly an explanation aligns with the model’s actual decision process, while acknowledging that\n",
        "no single metric captures explanation quality in a complete or reliable manner. We therefore use\n",
        "AOPC\\_MoRF as our **primary fidelity metric**, interpreted with transparency about its limitations\n",
        "and with a focus on relative — not absolute — comparisons.\n"
      ],
      "metadata": {
        "id": "SWJ6pEWyEsxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM Implementation:"
      ],
      "metadata": {
        "id": "vWxDr03vGNEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##now we implement the Method for Grad-CAM"
      ],
      "metadata": {
        "id": "tUMWrtbnGHm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "cv36WG9IGRv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmoothGrad Implementation:"
      ],
      "metadata": {
        "id": "-m_freq0GoiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SmoothGrad Implementation:\n"
      ],
      "metadata": {
        "id": "FpznWOLeGYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "rfmbFxmuGeXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LRP Implementation:"
      ],
      "metadata": {
        "id": "TN6n63OgGsPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LRP"
      ],
      "metadata": {
        "id": "36NU7lpQGjtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results:"
      ],
      "metadata": {
        "id": "HYTV8nu_GxKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison:"
      ],
      "metadata": {
        "id": "dHpsAGKPGzKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### all the metrics in one output"
      ],
      "metadata": {
        "id": "YMLCk2vQG2IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "From the plots, we can see that the --- intepretation method is optimal in maximising the $AOPC_{MoRF}(x)$ score over our batch of images. This shows that --- , under the CNN architecture defined in the previous sections, effectively highlights the relevant regions on average over our given batch of images.\n",
        "\n",
        "This interpretation may prove to be inferior in relevance ranking if we look at the whole set of images, but due to limitations in computational power for this project we only conduct analysis on the batch of images.\n",
        "\n",
        "Another important emphasis is, this investigation in reliability of our methods of interpretation are not based on ground-truth of tumor detection and the superior method could be more easily discerned with medically supervised segmentation masks on the tumor [7]."
      ],
      "metadata": {
        "id": "MTVtZtPqG63d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "[1]Lipton, Z.C., 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3), pp.31-57\n",
        "\n",
        "[2] Tomsett, Richard, et al. \"Sanity checks for saliency metrics.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.\n",
        "\n",
        "[3]Adebayo, Julius, et al. \"Sanity checks for saliency maps.\" Advances in neural information processing systems 31 (2018).\n",
        "\n",
        "[4]https://medium.com/@bijil.subhash/explainable-ai-saliency-maps-89098e230100\n",
        "\n",
        "[5]Arras, Leila, et al. \"Explaining recurrent neural network predictions in sentiment analysis.\" arXiv preprint arXiv:1706.07206 (2017).\n",
        "\n",
        "[6]Alvarez-Melis, David, and Tommi S. Jaakkola. \"On the robustness of interpretability methods.\" arXiv preprint arXiv:1806.08049 (2018).\n",
        "\n",
        "[7]Arun, Nishanth, et al. \"Assessing the trustworthiness of saliency maps for localizing abnormalities in medical imaging.\" Radiology: Artificial Intelligence 3.6 (2021): e200267."
      ],
      "metadata": {
        "id": "CuAd5M1Pa__E"
      }
    }
  ]
}