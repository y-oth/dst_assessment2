{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6qQIuLwqbHpF0MlWu5ZHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-oth/dst_assessment2/blob/main/report/02-SmoothGradIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SmoothGrad**\n",
        "\n",
        "Due to the limitations of Vanilla Gradient (noisy and unstable), we need to explore other methods. SmoothGrad, introduced by Smilkov et al (2017)[4], is a stronger pixel attribution method. Rather than computing a single gradient $\\nabla_x f_c(x)$ per image $x$, it adds Gaussian noise to duplicates of $x$, then averages over their Vanilla Gradients, in essence 'smoothing' out the high-frequency derivative fluctuations.[2]\n",
        "\n",
        "Given input image\n",
        "\n",
        "$$x \\in [0,1]^{C \\times H \\times W}$$,\n",
        "\n",
        "SmoothGrad creates an ensemble of noisy samples\n",
        "\n",
        "$$x_i = x + ϵ$$  where   $$ϵ \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n",
        "We have parameters $N$ - the number of samples in the ensemble - and $\\sigma$ - the noise level. Each $x_i$ is considered independently, identically distributed.\n",
        "\n",
        "The \"ideal\" parameter values suggested by Smilkov et al is a noise level of around 10-20% (as a proportion of the dynamic range of input features) and the sample number is $N \\approx 50$, as results tend to diminish past that point.\n",
        "\n",
        "Building from VanillaGrad, SmoothGrad computes the vanilla gradient of the predicted class for each sample $x_i$:\n",
        "\n",
        "$$G_i(x) = \\nabla_x f_c(x_i) \\in ℜ^{C \\times H \\times W}$$,\n",
        "\n",
        "then averages over their absoloute values:\n",
        "\n",
        "$$S_{\\text{SG}}(x) = \\frac{1}{N}\\sum_{i=1}^N \\vert G_i \\vert.$$\n",
        "\n",
        "As before, we then aggregate across channels and normalise to $[0,1]$:\n",
        "\n",
        "$$S_{\\text{SG}}(i,j)=\\max_{c \\in \\{1, \\dots, K\\}}\\vert S_{\\text{SG}}\\vert,$$\n",
        "\n",
        "$$S = \\frac{S - \\min(S)}{\\max(S) - \\min(S)}.$$\n",
        "\n",
        "**Advantages of SmoothGrad over Vanilla Grad**\n",
        "\n",
        "1. Removes high frequency noise by aggregating across noisy samples.\n",
        "\n",
        "2. Reinforces consistent structures across samples.\n",
        "\n",
        "3. Stabilises around ReLU boundaries, producing smoother saliency maps.\n",
        "\n",
        "**How does SmoothGrad stabilise around ReLU boundaries?**\n",
        "\n",
        "Since SmoothGrad computes the average of many Vanilla gradients from slightly augmented inputs $x_i$, when an original input $x$ is close to $0$, ReLU will class some samples as 'active' and some as 'inactive'.\n",
        "\n",
        "Therefore, when we average across these gradients, we get a proportional value $\\in$ [$0,1$] rather than a binary decision. In essence this is smoothing out gradient fluctuations and leads to a smoother, stabler saliency map. [3]"
      ],
      "metadata": {
        "id": "RnJIe7XUcmBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mathematical Proof**[1]\n",
        "\n",
        "The main benefit of SmoothGrad is its ability to reduce noise in our saliency maps. Here we will explore mathematically why this is true.\n",
        "\n",
        "1. We use a Taylor Series Expansion to prove that SmoothGrad's averaging method produces an average gradient approximately equal to the true gradient:\n",
        "\n",
        "Let $g(x) = f_c(x)$ be the logit of predicted class $c$.\n",
        "\n",
        "Considering a second-order Taylor expansion or sampe $x_i = x + \\epsilon $ around $x$:\n",
        "\n",
        "$$g(x + ϵ) ≈ g(x) + \\nabla g(x)^\\top \\epsilon + \\frac{1}{2}\\epsilon^\\top H_g(x)\\epsilon$$\n",
        "\n",
        "where $H_g(x)$ is the Hessian of $g$ at $x$.\n",
        "\n",
        "If we differentiate wrt. $x$:\n",
        "\n",
        "$$\\nabla_x g(x+\\epsilon) \\approx \\nabla g(x)+H_g(x)\\epsilon.$$\n",
        "\n",
        "Here\n",
        "$\\nabla_x g(x)$ is the true gradient and $H_g(x)\\epsilon$ is a noise term introduced by $\\epsilon$.\n",
        "\n",
        "Now we take the expectation noting that $\\mathbb{E}[ϵ] = 0$ since $ϵ$ is normally distributed with mean $0$:\n",
        "\n",
        "$$\\mathbb{E}[\\nabla_x g(x+\\epsilon)] \\approx \\mathbb{E}[\\nabla g(x)+ H_g(x)\\epsilon]$$\n",
        "\n",
        "$$\\approx \\nabla_x g(x)+ H_g(x)\\mathbb{E}[ϵ]$$\n",
        "\n",
        "$$ \\approx \\nabla_x g(x).$$\n",
        "\n",
        "Therefore, we have proven that taking the average over an ensemble of Gaussian noise injected samples does not distort the true gradient.\n",
        "\n",
        "2. SmoothGrad reduces the sample variance.\n",
        "\n",
        "Our averaged gradient is:\n",
        "\n",
        "$$\\bar{G}_N = \\frac{1}{N} \\sum_{i=1}^{N} G_i$$\n",
        "\n",
        "Due to the Gaussian noise being normally distributed, with each $\\epsilon_i$ and therefore $G_i$ assumed i.i.d, we can say that\n",
        "\n",
        "$$\\text{Var}[\\bar{G}_N]= \\frac{1}{N^2} \\sum_{i=1}^{N}{Var}[G_i]=\\frac{1}{N}\\text{Var}[G_i]$$\n",
        "\n",
        "Therefore, as $N$ increases, the variance of the gradient estimate decreases linearly with number of samples $N$.\n",
        "\n",
        "Overall, SmoothGrad gives the true gradient values of Vanilla Grad with lower variance and therefore less noise.\n"
      ],
      "metadata": {
        "id": "0GLGe_oNHfuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "[1] https://www.emergentmind.com/topics/smoothgrad-technique\n",
        "\n",
        "[2] https://users.cs.fiu.edu/~sjha/class2023/Lecture4/Slides/2017SmoothGrad.pdf\n",
        "\n",
        "[3] https://christophm.github.io/interpretable-ml-book/pixel-attribution.html\n",
        "\n",
        "[4] https://arxiv.org/pdf/1706.03825\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOgD7pG-brQk"
      }
    }
  ]
}
