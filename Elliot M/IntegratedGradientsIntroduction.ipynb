{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c497ca6",
   "metadata": {},
   "source": [
    "# Integrated Gradients: An interpretation method for deep learning models\n",
    "\n",
    "Here is a brief overview of Integrated Gradients, a method we use to interpret our CNN model predictions. The main motivation behind Integrated Gradients is that when we use a vanilla gradient method to attribute feature importance, we run into the problem of *saturation*. If the function has plateaued, the gradients will be close to zero, however the feature may still be important. Integrated Gradients solves this problem by considering the path from a baseline input (e.g. all zeros) to the actual input, and integrating the gradients along this path. This way, we capture the total change in the output as we move from the baseline to the actual input, providing a more accurate attribution of feature importance. Another problem that vanilla gradient methods face is that they can be highly sensitive to noise: a small perturbation in the input can lead to large changes in the gradient, making the attributions unstable. Since Integrated Gradients considers the entire path from the baseline to the input, it averages out these small perturbations, leading to more stable and reliable attributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c97c6",
   "metadata": {},
   "source": [
    "## Integrated Gradients Method (Sundararajan et al., 2017)\n",
    "\n",
    "Given a model $ F: \\mathbb{R}^n \\to [0, 1] $ and an input $x \\in \\mathbb{R}^n $, the Integrated Gradients for the $i^{th} $ feature is defined as:\n",
    "\n",
    "$$IG_i(x) = (x_i - x'_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha \\times (x - x'))}{\\partial x_i} d\\alpha$$\n",
    "\n",
    "where $ x' $ is the baseline input (e.g., all zeros), and $ \\alpha $ is a scaling factor that varies from 0 to 1. \n",
    "\n",
    "In practice this integral is approximated using a Riemann sum:\n",
    "\n",
    "$$IG_i(x) \\approx (x_i - x'_i) \\times \\frac{1}{m} \\sum_{k=1}^{m} \\frac{\\partial F(x' + \\frac{k}{m} \\times (x - x'))}{\\partial x_i}$$\n",
    "\n",
    "where $ m $ is the number of steps in the approximation. A larger $ m $ leads to a more accurate approximation of the integral. Generally $m$ is chosen between 20 and 300, in our case we use $m=50$ due to computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67aa9ba",
   "metadata": {},
   "source": [
    "### Properties of Integrated Gradients\n",
    "\n",
    "In the original paper by Sundararajan et al. (2017), the authors prove that Integrated Gradients satisfy two important properties:\n",
    "\n",
    "1. **Sensitivity**: If the input differs from the baseline in only one feature and the model's output changes, then the attribution for that feature should be non-zero. This ensures that important features are correctly identified.\n",
    "\n",
    "2. **Implementation Invariance**: If two models are functionally equivalent (i.e., they produce the same output for all inputs), then their attributions should also be identical. This property ensures that the attribution method is robust to different implementations of the same function.\n",
    "\n",
    "Intuitively, Integrated Gradients can be thought of as walking from the baseline input to the full input, and accumulating the gradients along the way. This captures the total contribution of each feature to the change in the model's output, providing a more comprehensive and reliable attribution compared to vanilla gradient methods, whilst avoiding saturation and noise issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9828ff7",
   "metadata": {},
   "source": [
    "## SG - IG: SmoothGrad Integrated Gradients \n",
    "\n",
    "One way we can try to improve the stability of Integrated Gradients is to combine it with SmoothGrad, resulting in the SmoothGrad Integrated Gradients (SG-IG) method. The idea behind SG-IG is to smooth the function first using SmoothGrad, and them apply Integrated Gradients to the smoothed function. This helps to reduce noise and improve the stability of the attributions.\n",
    "\n",
    "If we define SmoothGrad as:\n",
    "\n",
    "$$SG(x) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla F(x + \\epsilon_i))$$\n",
    "\n",
    "where $ N $ is the number of noisy samples, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) $ is Gaussian noise with mean 0 and variance $ \\sigma^2 $, then the SG-IG for the $ i^{th} $ feature is defined as:\n",
    "\n",
    "$$SG\\text{-}IG_i(x) = \\frac{1}{N} \\sum_{j=1}^{N} IG_i(x + \\epsilon_j)$$\n",
    "\n",
    "where $ IG_i(x + \\epsilon_j) $ is the Integrated Gradients for the noisy input $ x + \\epsilon_j $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa746fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
