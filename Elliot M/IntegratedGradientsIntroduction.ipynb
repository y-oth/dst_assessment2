{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c497ca6",
   "metadata": {},
   "source": [
    "# Integrated Gradients: An interpretation method for deep learning models\n",
    "\n",
    "Here is a brief overview of Integrated Gradients, a method we use to interpret our CNN model predictions. The main motivation behind Integrated Gradients is that when we use a vanilla gradient method to attribute feature importance, we run into the problem of *saturation*. Gradient saturation occurs when the model output stops changing even though the input is still important. For example, when the model has \"confident\" activations, the gradient can plateau, and the values will be close to zero, however the feature may still be important. Integrated Gradients solves this problem by considering the path from a baseline input (e.g. all zeros) to the actual input, and integrating the gradients along this path. Intuitively, we can think of this as starting at an extremely blurry or grey image, and at each step the image sharpens until we see the full resolution input. This way, we capture the total change in the output as we move from the baseline to the actual input, providing a more accurate attribution of feature importance. Another problem that vanilla gradient methods face is that they can be highly sensitive to noise: a small perturbation in the input can lead to large changes in the gradient, making the attributions unstable. Since Integrated Gradients considers the entire path from the baseline to the input, it averages out these small perturbations, leading to more stable and reliable attributions. This method was introduced by Sundararajan et al. [1] and has been widely adopted for interpreting deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c97c6",
   "metadata": {},
   "source": [
    "## Integrated Gradients Method (Sundararajan et al., 2017)\n",
    "\n",
    "Given a model $ F: \\mathbb{R}^n \\to [0, 1] $ and an input $x \\in \\mathbb{R}^n $, the Integrated Gradients for the $i^{th} $ feature is defined as:\n",
    "\n",
    "$$IG_i(x) = (x_i - x'_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha \\times (x - x'))}{\\partial x_i} d\\alpha$$\n",
    "\n",
    "where $ x' $ is the baseline input (e.g., all zeros), and $ \\alpha $ is a scaling factor that varies from 0 to 1. \n",
    "\n",
    "In practice this integral is approximated using a Riemann sum:\n",
    "\n",
    "$$IG_i(x) \\approx (x_i - x'_i) \\times \\frac{1}{m} \\sum_{k=1}^{m} \\frac{\\partial F(x' + \\frac{k}{m} \\times (x - x'))}{\\partial x_i}$$\n",
    "\n",
    "where $ m $ is the number of steps in the approximation. A larger $ m $ leads to a more accurate approximation of the integral. Generally $m$ is chosen between 20 and 300, in our case we use $m=50$ due to computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67aa9ba",
   "metadata": {},
   "source": [
    "### Properties of Integrated Gradients\n",
    "\n",
    "In the original paper by Sundararajan et al. (2017), the authors prove that Integrated Gradients satisfy two important properties:\n",
    "\n",
    "1. **Sensitivity**: If the input differs from the baseline in only one feature and the model's output changes, then the attribution for that feature should be non-zero. This ensures that important features are correctly identified.\n",
    "\n",
    "2. **Implementation Invariance**: If two models are functionally equivalent (i.e., they produce the same output for all inputs), then their attributions should also be identical. This property ensures that the attribution method is robust to different implementations of the same function.\n",
    "\n",
    "Intuitively, Integrated Gradients can be thought of as walking from the baseline input to the full input, and accumulating the gradients along the way. This captures the total contribution of each feature to the change in the model's output, providing a more comprehensive and reliable attribution compared to vanilla gradient methods, whilst avoiding saturation and noise issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9828ff7",
   "metadata": {},
   "source": [
    "## SG - IG: SmoothGrad Integrated Gradients \n",
    "\n",
    "One way we can try to improve the stability of Integrated Gradients is to combine it with SmoothGrad, resulting in the SmoothGrad Integrated Gradients (SG-IG) method. The idea behind SG-IG is to smooth the function first using SmoothGrad, and them apply Integrated Gradients to the smoothed function. This helps to reduce noise and improve the stability of the attributions.\n",
    "\n",
    "If we define SmoothGrad as:\n",
    "\n",
    "$$SG(x) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla F(x + \\epsilon_i))$$\n",
    "\n",
    "where $ N $ is the number of noisy samples, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) $ is Gaussian noise with mean 0 and variance $ \\sigma^2 $, then the SG-IG for the $ i^{th} $ feature is defined as:\n",
    "\n",
    "$$SG\\text{-}IG_i(x) = \\frac{1}{N} \\sum_{j=1}^{N} IG_i(x + \\epsilon_j)$$\n",
    "\n",
    "where $ IG_i(x + \\epsilon_j) $ is the Integrated Gradients for the noisy input $ x + \\epsilon_j $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea885a",
   "metadata": {},
   "source": [
    "# Intuition of interpolation between baseline and input\n",
    "Here is a simple illustration of the interpolation process between a baseline image (all zeros) and an actual input image using Integrated Gradients. The images below show the gradual transition from the baseline to the input, and for each step, we compute the gradient of the model's output with respect to the input. By integrating these gradients along the path, we can attribute the importance of each pixel in the input image to the model's prediction.\n",
    "\n",
    "This image was taken from the medium article [3] which is an excellent explainer for Integrated Gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e399a88",
   "metadata": {},
   "source": [
    "![IG path](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf0efa8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3319-3328). JMLR. org.\n",
    "\\\\\n",
    "[2] Smilkov, D., Thorat, N., Kim, B., Vi√©gas, F., & Wattenberg, M. (2017). Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825.\n",
    "\n",
    "[3] Piro, K. (2020). XAI Methods: Integrated Gradients. Medium. https://medium.com/@kemalpiro/xai-methods-integrated-gradients-6ee1fe4120d8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa746fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
